{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcvAP5l2uCQS",
        "outputId": "819719b3-1fe1-41c6-eabc-70cee8e30081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting efficient-apriori\n",
            "  Downloading efficient_apriori-2.0.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: efficient-apriori\n",
            "Successfully installed efficient-apriori-2.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install efficient-apriori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAchFMZoR4xU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Lf4HsAzRPL"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "## Base/Default Libraries:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import efficient_apriori as ea\n",
        "from scipy import stats\n",
        "\n",
        "## Utilities:\n",
        "from numbers import Number\n",
        "from google.colab import files\n",
        "import io\n",
        "import math\n",
        "\n",
        "\n",
        "## One-Hot Encoder:\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "## Model:\n",
        "import xgboost as xgb\n",
        "\n",
        "## Pickle & Saving objects onto memory:\n",
        "import pickle\n",
        "\n",
        "## Testing:\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLA1sZIbC5ym"
      },
      "outputs": [],
      "source": [
        "def who_is_applying(data: pd.DataFrame, rule: ea.Rule):\n",
        "  pass\n",
        "  # df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_g85AHGySmj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esgu0YdBxKP-"
      },
      "outputs": [],
      "source": [
        "def removeUniqueColumns(data: pd.DataFrame, percentage: float) -> tuple[pd.DataFrame, list]\n",
        "  rows_len = len(data.index)\n",
        "  columns_removed = []\n",
        "  for c in data.columns:\n",
        "    unique_values = data[c].nunique()\n",
        "    if unique_values >= percentage * rows_len: # Meaning, %percentage of the rows having unique values.\n",
        "      columns_removed.append(c)\n",
        "  data = data.drop(columns_removed, axis=1) # Dropping that data, because it's too unique-y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A3KznBKvFiC"
      },
      "outputs": [],
      "source": [
        "def getNullifiedColumns(data: pd.DataFrame, percentage: float):\n",
        "  missing_values_percentage = data.isnull().mean()\n",
        "  nullified_columns = missing_values_percentage[missing_values_percentage >= percentage].index\n",
        "  return nullified_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLiYqKWRx2w9"
      },
      "outputs": [],
      "source": [
        "def get_rid_nullified(data: pd.DataFrame, percentage: float) -> pd.DataFrame:\n",
        "  drop_these = getNullifiedColumns(data, percentage)\n",
        "  return data.drop(drop_these, axis=\"columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIP8b1wDIZCr"
      },
      "outputs": [],
      "source": [
        "def get_rid_incrementals(data: pd.DataFrame, percentage: Number = 1) -> pd.DataFrame:\n",
        "  if percentage > 1:\n",
        "    percentage = 1\n",
        "  drop_these = []\n",
        "  num_rows = len(data.index)\n",
        "  for c in data.columns:\n",
        "    if num_rows * percentage == data[c].nunique(dropna=False):\n",
        "      drop_these.append(c)\n",
        "  return data.drop(drop_these, axis=\"columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUV1b31yuNis"
      },
      "outputs": [],
      "source": [
        "class ColumnDivider():\n",
        "  def __init__(self, data: pd.DataFrame, target_feature: str, maximum_uniques: int):\n",
        "    self.numeric_columns = data.dtypes[(data.dtypes == \"float64\") | (data.dtypes == \"int64\")].index.tolist()\n",
        "    # maximum_uniques The number of unique values to be considered 'very numerical'.\n",
        "    self.very_numerical = [nc for nc in self.numeric_columns if data[nc].nunique() > maximum_uniques]\n",
        "    self.categorical_columns = [c for c in data.columns if c not in self.numeric_columns]  # and full_train_data[c].nunique() <= 5]\n",
        "    self.ordinals = list(set(self.numeric_columns) - set(self.very_numerical))\n",
        "    self.numeric_columns.remove(target_feature)\n",
        "  def get_vn(self):\n",
        "    return self.very_numerical # Returns all the columns that are very numerical (more than 10 unique values).\n",
        "  def get_cat(self):\n",
        "    return self.categorical_columns # Returns all the columns that are categorical. (Categories..)\n",
        "  def get_ord(self):\n",
        "    return self.ordinals # Returns all the columns that are ordinal (Less than or eql to 10 unique values).\n",
        "  def get_num(self):\n",
        "    return self.numeric_columns # Returns all the columns that are numeric (ordinal + very numerical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB5iTnfHxwrd"
      },
      "outputs": [],
      "source": [
        "def fill_null_model(data: pd.DataFrame, data_info: ColumnDivider):\n",
        "  # Decided we are giving mean for very numerical, mode for ordinal (less than 10 features), and 'nan' new category for categorical.\n",
        "  categorical_columns = data_info.get_cat()\n",
        "  ord_columns = data_info.get_ord()\n",
        "  vnum_columns = data_info.get_vn()\n",
        "  all_cols = data.columns\n",
        "  for c in categorical_columns:\n",
        "    if c not in all_cols:\n",
        "      continue\n",
        "    data[c] = pd.Categorical(data[c])\n",
        "    data[c] = data[c].cat.add_categories(\"nan\")\n",
        "    data[c] = data[c].fillna(\"nan\") # replacing na to the category 'na'.\n",
        "  for c in ord_columns:\n",
        "    if c not in all_cols:\n",
        "      continue\n",
        "    if data[c].mode().tolist():\n",
        "      data[c] = data[c].fillna(data[c].mode()[0])\n",
        "    else:\n",
        "      data[c] = data[c].fillna(0) # We can't really do anything else, except using 'get_rid_nullified'.\n",
        "      \n",
        "  for c in vnum_columns:\n",
        "    if c not in all_cols:\n",
        "      continue\n",
        "    data[c] = data[c].fillna(data[c].mean(skipna=True)) # replacing na to mean of the values.\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw33VwPSuNvJ"
      },
      "outputs": [],
      "source": [
        "class MostCorrelated():\n",
        "  most_correlated = {}\n",
        "  binning = {}\n",
        "  tf = \"\"\n",
        "  di = None\n",
        "  lists_len = 0\n",
        "  data_info = None\n",
        "  def __init__(self, data: pd.DataFrame, target_feature: str, num_correlated: int, data_info: ColumnDivider):\n",
        "\n",
        "    self.tf = target_feature\n",
        "    self.di = data_info\n",
        "    if num_correlated >= len(data.columns) - 1:\n",
        "      num_correlated = 10\n",
        "    self.lists_len = num_correlated\n",
        "    self.data_info = data_info\n",
        "\n",
        "    cat_cols, vnum_cols, num_cols, ord_cols = data_info.get_cat(), data_info.get_vn(), data_info.get_num(), data_info.get_ord()\n",
        "    # 2. Fill with a new 'na' category:\n",
        "    for c in cat_cols:\n",
        "      data[c] = pd.Categorical(data[c])\n",
        "      data[c] = data[c].cat.add_categories(\"nan\")\n",
        "      data[c] = data[c].fillna(\"nan\")\n",
        "\n",
        "    # binning is in the object.\n",
        "    for c in vnum_cols:\n",
        "        try:\n",
        "          data[c + '_binned'], self.binning[c] = pd.qcut(data[c], 5, labels=[\"very low\", \"low\", \"medium\", \"high\", \"very high\"], retbins=True)\n",
        "        except:\n",
        "            # sometimes for highly skewed data, we cannot perform qcut as most quantiles are equal\n",
        "          data[c + '_binned'], self.binning[c] = pd.cut(data[c], 5, labels=[\"very low\", \"low\", \"medium\", \"high\", \"very high\"], retbins=True)\n",
        "        finally:\n",
        "          self.binning[c] = np.concatenate(([-np.inf], self.binning[c][1:-1], [np.inf]))\n",
        "          data[c + '_binned'] = pd.Categorical(data[c + '_binned'])\n",
        "          data[c + '_binned'] = data[c + '_binned'].cat.add_categories(\"nan\")\n",
        "          data[c + '_binned'] = data[c + '_binned'].fillna(\"nan\")\n",
        "          # data[c].replace(np.nan, \"nan\", inplace=True)\n",
        "          # data.loc[data[c].isna(), c + '_binned'] = \"nan\"\n",
        "\n",
        "    # Binned very numericals! But didn't change v_numerical's nulls to appropriate.. now its np.nan\n",
        "\n",
        "\n",
        "    for c in ord_cols:\n",
        "      data[c + '_binned'] = pd.Categorical(data[c])\n",
        "      data[c + '_binned'] = data[c + '_binned'].cat.add_categories(\"nan\").fillna(\"nan\")\n",
        "\n",
        "    ord_cols_binned = [c + '_binned' for c in ord_cols]\n",
        "    vnum_cols_binned = [c + '_binned' for c in vnum_cols]\n",
        "    check_corr_list = list()\n",
        "    check_corr_list.extend(ord_cols_binned)\n",
        "    check_corr_list.extend(vnum_cols_binned)\n",
        "    check_corr_list.extend(cat_cols)\n",
        "    self.new_col_list = check_corr_list\n",
        "    set_corr = {}\n",
        "    for c_main in check_corr_list:\n",
        "      list_p_vals = []\n",
        "      for c_second in check_corr_list:\n",
        "        if c_main != c_second:\n",
        "          contingency_table = pd.crosstab(data[c_main],data[c_second])\n",
        "          c, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "          list_p_vals.append(p)\n",
        "        else:\n",
        "          list_p_vals.append(1.0)\n",
        "      idx = np.argpartition(np.array(list_p_vals), 10) # Getting the 10 largest elements.\n",
        "      set_corr[c_main] = idx[:10]\n",
        "\n",
        "    for k, v in set_corr.items():\n",
        "      self.most_correlated[k] = [check_corr_list[i] for i in v]\n",
        "    # most_correlated = {k: [check_corr_list[i] for i in v] for k, v in set_corr.items()}\n",
        "  def getMap(self) -> dict:\n",
        "    return self.most_correlated\n",
        "  def getMostCorrelated(self, feature: str) -> list:\n",
        "    return self.most_correlated.get(feature) # Returns null if it doesn't exists.\n",
        "  def getBinnings(self) -> dict:\n",
        "    return self.binning\n",
        "  def getBinningColumns(self) -> list:\n",
        "    return self.new_col_list\n",
        "  def getTarget(self) -> str:\n",
        "    return self.tf\n",
        "  def processTest(self, data: pd.DataFrame, len_test: int, min_supp: float = 0.1, min_conf: float = 0.75):\n",
        "    cat_cols, vnum_cols, num_cols, ord_cols = self.data_info.get_cat(), self.data_info.get_vn(), self.data_info.get_num(), self.data_info.get_ord()\n",
        "    # 2. Fill with a new 'na' category:\n",
        "    for c in cat_cols:\n",
        "      data[c] = pd.Categorical(data[c])\n",
        "      data[c] = data[c].cat.add_categories(\"nan\")\n",
        "      data[c] = data[c].fillna(\"nan\")\n",
        "    # data[cat_cols] = data[cat_cols].fillna(\"nan\") # Success! (Categorical doesnt have null values anymore!)\n",
        "    for c in vnum_cols:\n",
        "      data[c + '_binned'] = pd.cut(data[c], self.binning[c], labels=[\"very low\", \"low\", \"medium\", \"high\", \"very high\"]) # This is probably the main giveaway for the error. TODO -fix.\n",
        "      data[c + '_binned'] = pd.Categorical(data[c + '_binned'])\n",
        "      data[c + '_binned'] = data[c + '_binned'].cat.add_categories(\"nan\")\n",
        "      data[c + '_binned'] = data[c + '_binned'].fillna(\"nan\")\n",
        "\n",
        "    for c in ord_cols:\n",
        "      data[c + '_binned'] = pd.Categorical(data[c])\n",
        "      data[c + '_binned'] = data[c + '_binned'].cat.add_categories(\"nan\").fillna(\"nan\")\n",
        "\n",
        "    # for c in ['LotFrontage_binned', 'MasVnrArea_binned', 'OpenPorchSF_binned', 'GarageArea_binned', 'TotalBsmtSF_binned', 'BsmtFinSF1_binned', 'SalePrice_binned', 'RoofMatl', 'Neighborhood', '1stFlrSF_binned']:\n",
        "\n",
        "    rule_list = list()\n",
        "    for c in self.new_col_list: # All the new categories: \n",
        "      records = data[self.most_correlated[c]].to_dict(orient='records')\n",
        "      transactions=[]\n",
        "      for r in records:\n",
        "          transactions.append(list(r.items()))\n",
        "      itemsets, rules = ea.apriori(transactions, min_support=min_supp, min_confidence=min_conf,output_transaction_ids=False)\n",
        "      rule_list.extend(rules)\n",
        "\n",
        "    \n",
        "    \n",
        "    return rule_list, data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEuQONJsN4eO"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "  model_name: str = \"\"\n",
        "  train_filename: str = \"\"\n",
        "  this_model: xgb.XGBRegressor = None # The xgb regressor model.\n",
        "  apriori_helper: MostCorrelated = None # MostCorrelated [change mc to self.apriori_helper].\n",
        "  data_info: ColumnDivider = None # ColumnDivider [change cd to self.data_info].\n",
        "  # original_data = None # Original train data.\n",
        "  # Should I keep information about mistakes? rows I mistaken in the past..\n",
        "  name_dict = set() # Connects between the name the user gives to the test, and it's path.\n",
        "  encoder: OneHotEncoder = None\n",
        "  err_margin = 0.05 # Make is so the user can enter it on it's own. todo\n",
        "  def __init__(self, enc: OneHotEncoder = None, data_info: ColumnDivider = None):\n",
        "    if enc is not None:\n",
        "      self.encoder = enc\n",
        "    if data_info is not None:\n",
        "      self.data_info = data_info\n",
        "  \n",
        "  def getModel(self) -> xgb.XGBRegressor:\n",
        "    return self.this_model\n",
        "\n",
        "  def createModel(self, name: str) -> bool:\n",
        "    self.model_name = name\n",
        "    self.tf = \"\"\n",
        "    print(\"Please upload your TRAIN file (.csv):\\n\")\n",
        "    data_uploaded = files.upload()\n",
        "    filenames_uploaded = list(data_uploaded.keys())\n",
        "\n",
        "    if len(filenames_uploaded) != 1 or not filenames_uploaded[0].endswith('.csv'):\n",
        "      print(\"\\nError. File is not a \\'.csv\\' file.\\n\")\n",
        "      return False\n",
        "    # Next up - just upload and create a new model, train the train file (after all preprocess needed) andreturn to options.\n",
        "    # true_data = pd.read_csv('/content/train.csv')\n",
        "    self.train_filename = filenames_uploaded[0]\n",
        "\n",
        "    original_data = pd.read_csv(io.StringIO(data_uploaded[self.train_filename].decode('utf-8')))\n",
        "\n",
        "    # Ask for a target feature, and check if there is such column:\n",
        "    target_feature = None\n",
        "    while target_feature is None:\n",
        "      target_feature = input(\"\\nPlease enter the target feature: \")\n",
        "      if target_feature not in original_data.columns:\n",
        "        target_feature = None\n",
        "\n",
        "    self.tf = target_feature # Save it into the model.\n",
        "    \n",
        "\n",
        "    # Consider what is the difference between Very numerical to Ordinal columns.\n",
        "    max_uniq = None\n",
        "    while max_uniq is None:\n",
        "      max_uniq = input(\"\\nEnter maximum number of uniques allowed for ordinal (Will fill NAN values with mode instead of mean): \")\n",
        "      max_uniq = int(max_uniq) if max_uniq.isdecimal() else None\n",
        "\n",
        "    # Consider how many features we are going to try to find common between, when facing errors.\n",
        "    best_correlated_features = None\n",
        "    while best_correlated_features is None:\n",
        "      best_correlated_features = input(\"\\nEnter number of features we will test against, when facing errors: \")\n",
        "      best_correlated_features = int(best_correlated_features) if best_correlated_features.isdecimal() else None\n",
        "\n",
        "\n",
        "    data_for_binning = original_data.copy(deep=True) # For binning.\n",
        "\n",
        "    if self.data_info is None:\n",
        "      self.data_info = ColumnDivider(data_for_binning, self.tf, max_uniq)\n",
        "    self.apriori_helper = MostCorrelated(data_for_binning, self.tf, best_correlated_features, self.data_info) # Will be needed for making apriori rules.\n",
        "    \n",
        "    categorical_columns = self.data_info.get_cat()\n",
        "\n",
        "\n",
        "    ## If in the end I will need to seperate the data to hot encode.\n",
        "    # copied_data_ohe = true_data.copy(deep=True)\n",
        "    data_for_ohe = original_data.copy(deep=True)\n",
        "\n",
        "    #### TESTING!!!\n",
        "    print('PoolQC' in data_for_ohe)\n",
        "    data_for_ohe_filled = fill_null_model(data_for_ohe, self.data_info) # Didnt need that??\n",
        "    print('PoolQC' in data_for_ohe_filled )\n",
        "    if self.encoder is None:\n",
        "      self.encoder=OneHotEncoder(sparse=False)\n",
        "      data_for_ohe_filled_encoded = pd.DataFrame(self.encoder.fit_transform(data_for_ohe_filled[categorical_columns]))\n",
        "    else:\n",
        "      data_for_ohe_filled_encoded = pd.DataFrame(self.encoder.transform(data_for_ohe_filled[categorical_columns]))\n",
        "\n",
        "    data_for_ohe_filled_encoded.columns = self.encoder.get_feature_names(categorical_columns)\n",
        "\n",
        "    data_for_ohe_filled.drop(categorical_columns ,axis=1, inplace=True)\n",
        "\n",
        "    OHE_data = pd.concat([data_for_ohe_filled, data_for_ohe_filled_encoded], axis=1)\n",
        "\n",
        "    self.this_model = xgb.XGBRegressor(n_estimators=100, random_state=0)\n",
        "    self.this_model.fit(OHE_data.drop(self.tf, axis=1), OHE_data[self.tf])\n",
        "    return True\n",
        "\n",
        "  def uploadTest(self) -> bool:\n",
        "    print(\"Please upload your TEST file (.csv):\\n\")\n",
        "    data_uploaded = files.upload()\n",
        "    filenames_uploaded = list(data_uploaded.keys())\n",
        "    if len(filenames_uploaded) != 1 or not filenames_uploaded[0].endswith('.csv'):\n",
        "      print(\"\\nError. File is not a \\'.csv\\' file.\\n\")\n",
        "      # return False\n",
        "    # That is okay if more than 1 model use the same test file.\n",
        "    self.name_dict.add(filenames_uploaded[0])\n",
        "\n",
        "    categorical_columns = self.data_info.get_cat()\n",
        "\n",
        "    # test_data = pd.read_csv('/content/' + filenames_uploaded[0]) # NO GOOD - get it from the uploaded data...\n",
        "    test_data = pd.read_csv(io.StringIO(data_uploaded[filenames_uploaded[0]].decode('utf-8')))\n",
        "\n",
        "    test_data_for_ohe_filled = fill_null_model(test_data, self.data_info)\n",
        "\n",
        "    test_data_for_ohe_filled_encoded = pd.DataFrame(self.encoder.transform(test_data_for_ohe_filled[categorical_columns]))\n",
        "\n",
        "    test_data_for_ohe_filled_encoded.columns = self.encoder.get_feature_names(categorical_columns)\n",
        "\n",
        "    test_data_for_ohe_filled.drop(categorical_columns ,axis=1, inplace=True)\n",
        "\n",
        "    OHE_test_data = pd.concat([test_data_for_ohe_filled, test_data_for_ohe_filled_encoded], axis=1)\n",
        "\n",
        "    predictions_test = self.this_model.predict(OHE_test_data)\n",
        "\n",
        "    # Prediction should be $modelname.$testname.txt\n",
        "    try:\n",
        "      with open('/content/' + self.model_name + '_' + filenames_uploaded[0] + '.txt', 'w') as f:\n",
        "        for idx, row_pred in enumerate(predictions_test):\n",
        "          if idx < len(predictions_test) - 1: # If not the final prediction..\n",
        "            f.write(str(row_pred) + '\\n')\n",
        "          else: # If it is the final prediction.. (w/o newline)\n",
        "            f.write(str(row_pred))\n",
        "    except:\n",
        "      print(\"\\nError. Couldn't write the prediction file (\\'.txt\\').\\n\")\n",
        "      return False\n",
        "    return True\n",
        "    \n",
        "  def uploadPrevResults(self) -> bool:\n",
        "    print(\"\\nTEST RESULT file should be a text file, with each row having \\nthe real target feature value of the compatible row in the data.\\n\")\n",
        "    print(\"Make sure the file does NOT end with a newline.\\n\")\n",
        "    print(\"Please upload your TEST RESULT file (.txt):\\n\")\n",
        "    data_uploaded = files.upload()\n",
        "    filenames_uploaded = list(data_uploaded.keys())\n",
        "    if len(filenames_uploaded) != 1 or not filenames_uploaded[0].endswith('.txt'):\n",
        "      print(\"\\nError. File is not a \\'.txt\\' file.\\n\")\n",
        "      # return False\n",
        "      return False\n",
        "    \n",
        "    test_filename = 'nan'\n",
        "    while test_filename == 'nan' or test_filename not in self.name_dict:\n",
        "      test_filename = input(\"Please enter a valid test filename:\\n\")\n",
        "    \n",
        "\n",
        "    \n",
        "    list_true_res = data_uploaded[filenames_uploaded[0]].decode(\"utf-8\").split('\\n')\n",
        "    \n",
        "    try:\n",
        "      with open('/content/' + self.model_name + '_' + test_filename + '.txt', \"r\") as f:\n",
        "        test_predictions = [line.rstrip() for line in f]\n",
        "    except:\n",
        "      print('Error. Failed to open saved prediction files.')\n",
        "    \n",
        "    if len(list_true_res) != len(test_predictions):\n",
        "      print('Error. Not equal number of predictions and true values.\\n')\n",
        "      # return False\n",
        "      return False\n",
        "    \n",
        "    true_vs_pred = list()\n",
        "    try:\n",
        "      for true_val, pred in zip(list_true_res, test_predictions):\n",
        "        true_vs_pred.append((float(true_val), float(pred)))\n",
        "    except:\n",
        "      print('Value Error. Couldn\\'t transfer one of the true values or prediction values to numeric value.\\n')\n",
        "      # return False\n",
        "      return False\n",
        "    \n",
        "    \n",
        "    # Continue to calculate errors (based on user first input on what is considered error), and do apriori and fit the model if found common itemsets~~\n",
        "\n",
        "    err_margin_list = [(abs(item[0] - item[1]) / item[0]) for item in true_vs_pred]\n",
        "    err_indices = list()\n",
        "    for idx, item in enumerate(err_margin_list):\n",
        "      if item > self.err_margin:\n",
        "        err_indices.append(idx)\n",
        "\n",
        "    \n",
        "    test_df = pd.read_csv('/content/' + test_filename)\n",
        "    # test_df = test_df.reset_index()\n",
        "    test_df = test_df.iloc[err_indices]\n",
        "\n",
        "    sale_price_list = list()\n",
        "    for i in err_indices:\n",
        "      sale_price_list.append(true_vs_pred[i][0])\n",
        "\n",
        "    test_df[self.tf] = sale_price_list # We are inserting the true values of the rows (received from the user).\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    # Maybe insert a column with their real value..?\n",
        "\n",
        "    test_rows_len = len(true_vs_pred)\n",
        "\n",
        "    rule_list, test_transformed = self.apriori_helper.processTest(test_df.copy(deep=True), test_rows_len)\n",
        "\n",
        "    rule_list = sorted(rule_list, key=lambda rule: (len(rule.lhs) + len(rule.rhs)) * rule.support) # The most lengthiest rule, which has high support (common itemset)\n",
        "    \n",
        "    row_atleast = min(math.ceil(0.2 * test_rows_len), len(err_indices)) # It will be the percentage from the test length we are willing to take, based on errors. 0.2 * len(test).\n",
        "\n",
        "    rules_atleast = math.ceil(0.1 * len(rule_list))\n",
        "\n",
        "    indices_rows = set()\n",
        "    \n",
        "\n",
        "    if row_atleast != len(err_indices):\n",
        "      for rule in rule_list:\n",
        "        rules_atleast = rules_atleast - 1\n",
        "        for index, row in test_transformed.iterrows():\n",
        "          if index in indices_rows:\n",
        "            continue\n",
        "          fail = False\n",
        "          for item in rule.rhs:\n",
        "            if row[item[0]] != item[1]:\n",
        "              fail = True\n",
        "              break\n",
        "          if fail == True:\n",
        "            continue\n",
        "          for item in rule.lhs:\n",
        "            if row[item[0]] != item[1]:\n",
        "              fail = True\n",
        "              break\n",
        "          if fail == False:\n",
        "            indices_rows.add(index)\n",
        "        if rules_atleast == 0:\n",
        "          break\n",
        "        if len(indices_rows) >= row_atleast:\n",
        "          break\n",
        "      indices_rows = list(indices_rows)\n",
        "    else:\n",
        "      indices_rows = []\n",
        "\n",
        "\n",
        "    if indices_rows:\n",
        "      insert_these = []\n",
        "      for index, row in test_df.iterrows():\n",
        "        if index in indices_rows:\n",
        "          insert_these.append(row.values)\n",
        "      test_df = pd.DataFrame(insert_these, columns = test_df.columns).reset_index() # take the rows from test_df that has the same indices as transformed (we took indices of rows that comply with our rules after all..)\n",
        "    else:\n",
        "      pass # take everything??\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    categorical_columns = self.data_info.get_cat()\n",
        "    \n",
        "\n",
        "    test_data_fit_filled = fill_null_model(test_df, self.data_info)\n",
        "\n",
        "\n",
        "    test_data_fit_filled_encoded = pd.DataFrame(self.encoder.transform(test_data_fit_filled[categorical_columns]))\n",
        "\n",
        "    test_data_fit_filled_encoded.columns = self.encoder.get_feature_names(categorical_columns)\n",
        "\n",
        "    test_data_fit_filled.drop(categorical_columns ,axis=1, inplace=True)\n",
        "\n",
        "\n",
        "    OHE_test_data_fit = pd.concat([test_data_fit_filled.reset_index(drop=True), test_data_fit_filled_encoded.reset_index(drop=True)], axis=1) # Changed into 'ignore_index=True'.\n",
        "    if 'index' in OHE_test_data_fit:\n",
        "      OHE_test_data_fit.drop('index', axis=1, inplace=True)\n",
        "\n",
        "    print(OHE_test_data_fit.head())\n",
        "\n",
        "    self.this_model = self.this_model.fit(OHE_test_data_fit.drop(self.tf, axis=1), OHE_test_data_fit[self.tf], xgb_model=self.this_model.get_booster())\n",
        "\n",
        "        \n",
        "            \n",
        "        \n",
        "      # Just check if a row consist with all of these features.\n",
        "    \n",
        "\n",
        "\n",
        "    # run the apriori on them. take only x rules by user's input.\n",
        "\n",
        "\n",
        "\n",
        "    # return True\n",
        "    return True\n",
        "  \n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJsCv154PWQy"
      },
      "outputs": [],
      "source": [
        "def our_project(enc: OneHotEncoder = None, data_info: ColumnDivider = None):\n",
        "  models = {} # Todo use that to contain all Models and their name\n",
        "  while True:\n",
        "    option = 0\n",
        "    while option < 1 or option > 4:\n",
        "      option = input(\"Please choose:\\n1. Create New Model.\\n2. Upload another test file for prediction.\\n3. Upload results for a previous test file.\\n4. Return a requested model.\\nEnter option: \")\n",
        "      option = int(option) if option.isdecimal() else 0\n",
        "    if option == 1:\n",
        "      model_name = \"nan\"\n",
        "      while model_name == \"nan\":\n",
        "        model_name = input(\"\\nEnter a name for the new model (except \\'nan\\'): \\n\")\n",
        "      models[model_name] = Model(enc, data_info)\n",
        "      if not models[model_name].createModel(model_name):\n",
        "        del models[model_name]\n",
        "        \n",
        "    elif option == 2:\n",
        "      model_name = \"nan\"\n",
        "      while model_name == \"nan\" or model_name not in models.keys():\n",
        "        model_name = input(\"\\nEnter the name of the model (except \\'nan\\'): \\n\")\n",
        "      models[model_name].uploadTest() # We dont care if it is failed or not.\n",
        "\n",
        "    elif option == 3:\n",
        "      model_name = \"nan\"\n",
        "      while model_name == \"nan\" or model_name not in models.keys():\n",
        "        model_name = input(\"\\nEnter the name of the model (except \\'nan\\'): \\n\")\n",
        "      # models[model_name].uploadPrevResults()\n",
        "      models[model_name].uploadPrevResults()\n",
        "    else:\n",
        "      model_name = \"nan\"\n",
        "      while model_name == \"nan\" or model_name not in models.keys():\n",
        "        model_name = input(\"\\nEnter the name of the model (except \\'nan\\'): \\n\")\n",
        "      return models[model_name].getModel()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "vZRMnyWNPzf-",
        "outputId": "845fbf30-d301-4f87-d7c9-94180754d517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your TRAIN file (.csv):\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1f641c1e-a346-4ea1-954d-2348cf77ccfa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1f641c1e-a346-4ea1-954d-2348cf77ccfa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving train1.csv to train1 (1).csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/pivot.py:188: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "[19:43:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ],
      "source": [
        "# def not_our_project(df_train, df_tests):\n",
        "def main(): # Our main shows us the difference between our solution and a normal training (training on all mistakes)\n",
        "  data = pd.read_csv('/content/train.csv')\n",
        "\n",
        "  data_info = ColumnDivider(data.copy(deep=True), 'SalePrice', 10)\n",
        "  categorical_columns = data_info.get_cat()\n",
        "\n",
        "  saved_train = data.sample(frac=0.05, random_state=200)\n",
        "  test = data.drop(saved_train.index)\n",
        "  saved_test1, saved_test2, saved_test3 = test.sample(frac=0.2 ,random_state=190), test.sample(frac=0.2 ,random_state=180), test.sample(frac=0.2 ,random_state=170)\n",
        "\n",
        "  saved_test1_answers, saved_test2_answers, saved_test3_answers = saved_test1['SalePrice'], saved_test2['SalePrice'], saved_test3['SalePrice']\n",
        "  saved_test1, saved_test2, saved_test3 = saved_test1.drop('SalePrice', axis=1), saved_test2.drop('SalePrice', axis=1), saved_test3.drop('SalePrice', axis=1)\n",
        "\n",
        "\n",
        "  data_filled = fill_null_model(data.copy(deep=True), data_info)\n",
        "\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "  data_filled_encoded = pd.DataFrame(encoder.fit_transform(data_filled[categorical_columns]))\n",
        "\n",
        "  data_filled_encoded.columns = encoder.get_feature_names(categorical_columns)\n",
        "\n",
        "  data_filled.drop(categorical_columns ,axis=1, inplace=True)\n",
        "\n",
        "  OHE_data = pd.concat([data_filled.reset_index(drop=True), data_filled_encoded.reset_index(drop=True)], axis=1)\n",
        "\n",
        "\n",
        "  train = OHE_data.sample(frac=0.5,random_state=200)\n",
        "  test = OHE_data.drop(train.index)\n",
        "\n",
        "  test1, test2, test3 = test.sample(frac=0.2 ,random_state=190), test.sample(frac=0.2 ,random_state=180), test.sample(frac=0.2 ,random_state=170)\n",
        "\n",
        "  test1_answers, test2_answers, test3_answers = test1['SalePrice'], test2['SalePrice'], test3['SalePrice']\n",
        "  test_final = test.copy(deep=True)\n",
        "  test_final.drop(test1.index, inplace = True, errors='ignore')\n",
        "  test_final.drop(test2.index, inplace = True, errors='ignore')\n",
        "  test_final.drop(test3.index, inplace = True, errors='ignore')\n",
        "  test1, test2, test3 = test1.drop('SalePrice', axis=1), test2.drop('SalePrice', axis=1), test3.drop('SalePrice', axis=1)\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  normal_model = xgb.XGBRegressor(n_estimators=100, random_state=0)\n",
        "\n",
        "  normal_model.fit(train.drop('SalePrice', axis=1), train['SalePrice'])\n",
        "\n",
        "\n",
        "  # Test1\n",
        "  prediction_normal_test1 = normal_model.predict(test1)\n",
        "\n",
        "  test1_answers_list = test1_answers.values.tolist()\n",
        "  \n",
        "  indices_list =list()\n",
        "  for i in range(len(test1_answers)):\n",
        "    if abs(prediction_normal_test1[i] - test1_answers_list[i]) / test1_answers_list[i] < 0.05:\n",
        "      indices_list.append(i)\n",
        "  \n",
        "  normal_model = normal_model.fit(test1.iloc[indices_list], test1_answers.iloc[indices_list], xgb_model=normal_model.get_booster())\n",
        "\n",
        "\n",
        "  # Test2\n",
        "  prediction_normal_test2 = normal_model.predict(test2)\n",
        "  test2_answers_list = test2_answers.values.tolist()\n",
        "  \n",
        "  indices_list = list()\n",
        "  for i in range(len(test2_answers)):\n",
        "    if abs(prediction_normal_test2[i] - test2_answers_list[i]) / test2_answers_list[i] < 0.05:\n",
        "      indices_list.append(i)\n",
        "  \n",
        "  normal_model = normal_model.fit(test2.iloc[indices_list], test2_answers.iloc[indices_list], xgb_model=normal_model.get_booster())\n",
        "\n",
        "  # Test3\n",
        "  prediction_normal_test3 = normal_model.predict(test3)\n",
        "  test3_answers_list = test3_answers.values.tolist()\n",
        "  \n",
        "  indices_list =list()\n",
        "  for i in range(len(test3_answers)):\n",
        "    if abs(prediction_normal_test3[i] - test3_answers_list[i]) / test3_answers_list[i] < 0.05:\n",
        "      indices_list.append(i)\n",
        "  \n",
        "  normal_model = normal_model.fit(test3.iloc[indices_list], test3_answers.iloc[indices_list], xgb_model=normal_model.get_booster())\n",
        "\n",
        "  final_predictions_normal = normal_model.predict(test_final.drop('SalePrice', axis=1)) # normal model.\n",
        "  true_values_test = test_final['SalePrice'].values.tolist()\n",
        "\n",
        "\n",
        "  count_failed = 0\n",
        "  for i in range(len(true_values_test)):\n",
        "    if abs(final_predictions_normal[i] - true_values_test[i]) / true_values_test[i] < 0.05:\n",
        "      count_failed = count_failed + 1\n",
        "    \n",
        "  normal_success = count_failed / len(true_values_test)\n",
        "\n",
        "\n",
        "  train = data.sample(frac=0.5,random_state=200)\n",
        "  test = data.drop(train.index)\n",
        "  test1, test2, test3 = test.sample(frac=0.2 ,random_state=190), test.sample(frac=0.2 ,random_state=180), test.sample(frac=0.2 ,random_state=170)\n",
        "\n",
        "  test1_answers, test2_answers, test3_answers = test1['SalePrice'].values.tolist(), test2['SalePrice'].values.tolist(), test3['SalePrice'].values.tolist()\n",
        "  test1, test2, test3 = test1.drop('SalePrice', axis=1), test2.drop('SalePrice', axis=1), test3.drop('SalePrice', axis=1)\n",
        "\n",
        "  \n",
        "  saved_train.to_csv(Path('/content/train1.csv'), index=False)\n",
        "  print(saved_train.columns)\n",
        "  \n",
        "  saved_test1.to_csv(Path('/content/test1.csv'), index=False)\n",
        "  saved_test2.to_csv(Path('/content/test2.csv'), index=False)\n",
        "  saved_test3.to_csv(Path('/content/test3.csv'), index=False)\n",
        "\n",
        "  try:\n",
        "    with open('/content/results_test1.txt', 'w') as f:\n",
        "      for idx, ans in enumerate(saved_test1_answers):\n",
        "        if idx < len(saved_test1_answers) - 1: # If not the final prediction..\n",
        "          f.write(str(ans) + '\\n')\n",
        "        else: # If it is the final prediction.. (w/o newline)\n",
        "          f.write(str(ans))\n",
        "  except:\n",
        "    print(\"\\nError. Couldn't write the prediction file (\\'.txt\\').\\n\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    with open('/content/results_test2.txt', 'w') as f:\n",
        "      for idx, ans in enumerate(saved_test2_answers):\n",
        "        if idx < len(saved_test2_answers) - 1: # If not the final prediction..\n",
        "          f.write(str(ans) + '\\n')\n",
        "        else: # If it is the final prediction.. (w/o newline)\n",
        "          f.write(str(ans))\n",
        "  except:\n",
        "    print(\"\\nError. Couldn't write the prediction file (\\'.txt\\').\\n\")\n",
        "\n",
        "  try:\n",
        "    with open('/content/results_test3.txt', 'w') as f:\n",
        "      for idx, ans in enumerate(saved_test3_answers):\n",
        "        if idx < len(saved_test3_answers) - 1: # If not the final prediction..\n",
        "          f.write(str(ans) + '\\n')\n",
        "        else: # If it is the final prediction.. (w/o newline)\n",
        "          f.write(str(ans))\n",
        "  except:\n",
        "    print(\"\\nError. Couldn't write the prediction file (\\'.txt\\').\\n\")\n",
        "\n",
        "\n",
        "  our_model = our_project(encoder, data_info)\n",
        "  our_predictions = our_model.predict(test_final.drop('SalePrice', axis=1))\n",
        "  print('our_pred\\n', our_predictions)\n",
        "  count_failed = 0\n",
        "  for i in range(len(true_values_test)):\n",
        "    if abs(our_predictions[i] - true_values_test[i]) / true_values_test[i] < 0.05:\n",
        "      count_failed = count_failed + 1\n",
        "\n",
        "  our_success = count_failed / len(true_values_test)\n",
        "\n",
        "  print('Our success: ', our_success, '\\nNormal Success: ', normal_success)\n",
        "# main()\n",
        "our_project()\n",
        "\n",
        "\n",
        "  \n",
        "      \n",
        "\n",
        "  \n",
        "\n",
        "  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
